{"docstore/metadata": {"147be114-6e64-4606-ab9c-727f38f96358": {"doc_hash": "f42ff1852f0073686844da42412b2adb9b5d6ff3a99c3f0ea4d6f7092959611b"}, "78e661ae-b3eb-45be-99cb-578ec0debb15": {"doc_hash": "d8c2ba61b2ae376462ff952693fd103a98f4306674d74dd301ee1c8335b35550"}, "a18b3dfb-2358-4d53-8858-dbafae882a2b": {"doc_hash": "a49cb31bd79aa695085735b26556e0d84c3bedb138aef5d0b4b4a3192a1e8c7d"}, "dcf90697-212e-4a7e-9910-c2659c06c401": {"doc_hash": "3a747e3fe1953b744fbd469498f7cef5506d59ee8b071f67de64ccbb34e68e90"}, "db1731c1-39e5-409a-a6a1-124d56f1c9d7": {"doc_hash": "d8eef011d912dae07ea6ccd36ba49f40b793930d280b7418748cf2b56d921b9c"}, "1ad4a6cb-a2da-4558-9d9d-83dbcb8e125e": {"doc_hash": "57fecee489551a94e45d52ad1581f0b10494fd46870f2500a5eb981a032d739b"}, "6a88dc97-8d0e-42f9-a2a4-222d373f7a56": {"doc_hash": "3d0732beea4eaf38c1685aa5a45a3f9bb0b452cc41e95514b47d6fed5327c98d"}, "c2db5b72-6ae8-49c3-a32b-14bd4925be2e": {"doc_hash": "d7ffc272396a3c00e1c6b2ef7639d57a9d4632bba1d59e614ca4ca5a7aa125d6"}, "a250b327-fb6f-4a03-8ef3-0e1cbfbf2c4f": {"doc_hash": "60203d9fc40b47a00a13729e7585d3a507e0bdb9deb740831a538b493e01ad0a"}, "95c59b4e-7eb7-4391-a5d8-911e5874e19b": {"doc_hash": "9c024b870affae45880d4ea3ccca67acc398251d84bd1efdebf6ed83dd9c314c"}, "a9deb209-06ed-4fda-8e46-a5d43ce9b2a1": {"doc_hash": "06a2700981d5d48c75b90524c1795d98a0fe67861029330330f498c7a360e040"}, "7421f293-a6fc-453d-84a5-0d219894eeda": {"doc_hash": "a7d1f4c9e7d80a70dd1a0f61d7295ce77fc2795c0d48de5e1c0b78dfea25f0f1"}, "ed7c1c6b-10fd-4cc9-ba23-4df720836825": {"doc_hash": "f40146bb876d087eefbe106b8eccd43b2b3bf28aab7f5e86336c118015d9a339"}, "effda0de-c169-4ba6-a1e8-db3793f30041": {"doc_hash": "1e17591bd5bbb926f148424778bf7897639d263c274308cd0ecdf2cbced12327"}, "7c7eddad-b587-4efa-86a2-43ae5b3bfaf5": {"doc_hash": "3b9abc181b441371f2fe2f9595cb85109c3abbb3799e6596d9485011ba07a9af"}, "89715caf-6735-4607-b3a2-a854ecc8c9d7": {"doc_hash": "c1f226d15865fe95197edf95150598ce1951236bd40a4fd8679fb56261806765"}, "a329cf5c-f7b9-4958-be14-a2eb7aff7221": {"doc_hash": "0426657841b340a4f3278446a577b42dd2e70bceb12b981febb746ad9cc03fe9"}, "bd17e473-6af2-411d-a0e3-0811abfc616c": {"doc_hash": "5d3fadaf2163c9feaa502124b1043f0dfbcb8e1b8035e0172e2d730dbf886270"}, "c6ec5370-d419-4bf8-8238-1399f040f019": {"doc_hash": "34a4c685d09d34c3a5d49213e47c8864cced03b6df80a903aa0cd088401f1103"}, "b695dc0e-24a0-4e0e-b7a5-73308ddf08e0": {"doc_hash": "55e4574598a5c35b41abcbe8fefae2550f5c9be413892f270a0ff13ffd6ea114"}, "995c9372-b1db-4f00-abee-b7e6d05abeee": {"doc_hash": "34aa697bf1e75e6864f0ac6f420577c0ee268903a02d891f56eb6735eb9804ba"}, "3e6520bd-457e-4508-8d1f-afa42713b628": {"doc_hash": "6979654e269a024e9729fffc72dba0840a045e4cca5cbfd8f74fe34cf59534b6"}, "2e8bad1c-437e-412d-a454-0705a14a0f9b": {"doc_hash": "8e4b2d4478b9268a3b8f7a68a680bf8d345e4f3ba5ccf2689494650a0258caf8"}, "52fcc4bb-6801-40e5-93ba-13e2696bf579": {"doc_hash": "59b93c6f7b390705866be2bf80199dd13bee014debfd4cdd97ff096d16f0c673"}, "60b32a50-fc63-4f10-8cb0-637b1a162155": {"doc_hash": "101f04ae6e8773e6cdf83a1d27b36aa671e1707450a7aac0d5f49c9acae39487"}, "0a0b8d18-6403-430b-9cfb-c745d54795dd": {"doc_hash": "4d576d713791a82e3038e880d46014d9876599ea091eddcf3c5e5851cc61a90a"}, "47c4d19c-d72c-4d84-848c-e6c7726c4bbf": {"doc_hash": "f42ff1852f0073686844da42412b2adb9b5d6ff3a99c3f0ea4d6f7092959611b", "ref_doc_id": "147be114-6e64-4606-ab9c-727f38f96358"}, "0cd1a1b5-d0f8-45a3-9337-8a471845d961": {"doc_hash": "198ea23396cb67dbf2aec00a75dc6c323bdb2f5bfe8b8a17375ff447d2da6118", "ref_doc_id": "78e661ae-b3eb-45be-99cb-578ec0debb15"}, "b0bf8545-494e-431c-ba5b-6483082b3164": {"doc_hash": "d7f888d18db9aa78287b39d3700a8dc1413e21f35af3b2470adaca9ab5990de9", "ref_doc_id": "78e661ae-b3eb-45be-99cb-578ec0debb15"}, "1d7fd207-e8a5-47c8-b203-fbc116caba2e": {"doc_hash": "a49cb31bd79aa695085735b26556e0d84c3bedb138aef5d0b4b4a3192a1e8c7d", "ref_doc_id": "a18b3dfb-2358-4d53-8858-dbafae882a2b"}, "ee7e21da-5822-4593-a744-1a90b5f86176": {"doc_hash": "3a747e3fe1953b744fbd469498f7cef5506d59ee8b071f67de64ccbb34e68e90", "ref_doc_id": "dcf90697-212e-4a7e-9910-c2659c06c401"}, "833fdb77-3416-4035-9efa-2825805532e3": {"doc_hash": "d8eef011d912dae07ea6ccd36ba49f40b793930d280b7418748cf2b56d921b9c", "ref_doc_id": "db1731c1-39e5-409a-a6a1-124d56f1c9d7"}, "a8fa3638-3ab8-465f-b227-97cdc5edcccf": {"doc_hash": "57fecee489551a94e45d52ad1581f0b10494fd46870f2500a5eb981a032d739b", "ref_doc_id": "1ad4a6cb-a2da-4558-9d9d-83dbcb8e125e"}, "3bef6d8f-73da-4fb4-ae56-75272e7215d1": {"doc_hash": "3d0732beea4eaf38c1685aa5a45a3f9bb0b452cc41e95514b47d6fed5327c98d", "ref_doc_id": "6a88dc97-8d0e-42f9-a2a4-222d373f7a56"}, "48d8a2d5-54e8-41b4-83f2-c40bf3d08d2b": {"doc_hash": "d7ffc272396a3c00e1c6b2ef7639d57a9d4632bba1d59e614ca4ca5a7aa125d6", "ref_doc_id": "c2db5b72-6ae8-49c3-a32b-14bd4925be2e"}, "c492d0c3-4461-4d67-8eb7-1b68d5532e94": {"doc_hash": "60203d9fc40b47a00a13729e7585d3a507e0bdb9deb740831a538b493e01ad0a", "ref_doc_id": "a250b327-fb6f-4a03-8ef3-0e1cbfbf2c4f"}, "59d67118-5878-4283-a2ae-ad62b1fa24a8": {"doc_hash": "9c024b870affae45880d4ea3ccca67acc398251d84bd1efdebf6ed83dd9c314c", "ref_doc_id": "95c59b4e-7eb7-4391-a5d8-911e5874e19b"}, "2c70108e-192b-4e44-8a03-1d42bea658cf": {"doc_hash": "06a2700981d5d48c75b90524c1795d98a0fe67861029330330f498c7a360e040", "ref_doc_id": "a9deb209-06ed-4fda-8e46-a5d43ce9b2a1"}, "6ba7d6df-b14c-490a-a084-d01253ffdfc2": {"doc_hash": "a7d1f4c9e7d80a70dd1a0f61d7295ce77fc2795c0d48de5e1c0b78dfea25f0f1", "ref_doc_id": "7421f293-a6fc-453d-84a5-0d219894eeda"}, "f8c4bee8-cba8-495c-88d2-3341191704f4": {"doc_hash": "f40146bb876d087eefbe106b8eccd43b2b3bf28aab7f5e86336c118015d9a339", "ref_doc_id": "ed7c1c6b-10fd-4cc9-ba23-4df720836825"}, "0b569a7e-88ab-450d-9450-aa3c47b07e0d": {"doc_hash": "ca417b4eb50954efda8b350881ebbd2194b7505affbdba20b4d2594028aa49f6", "ref_doc_id": "effda0de-c169-4ba6-a1e8-db3793f30041"}, "5a0cfe07-b635-4fed-a364-57ed20036d10": {"doc_hash": "adb70fa7022fbe71ec500fb10acd543f4bf12f4ced4e8359f9b8eddd607a32f9", "ref_doc_id": "effda0de-c169-4ba6-a1e8-db3793f30041"}, "ff2ff41a-dfe0-4d23-98cf-ebc90da29ecd": {"doc_hash": "3b9abc181b441371f2fe2f9595cb85109c3abbb3799e6596d9485011ba07a9af", "ref_doc_id": "7c7eddad-b587-4efa-86a2-43ae5b3bfaf5"}, "ee3710e9-3993-4b3c-ae47-2a2b43b82ca4": {"doc_hash": "c1f226d15865fe95197edf95150598ce1951236bd40a4fd8679fb56261806765", "ref_doc_id": "89715caf-6735-4607-b3a2-a854ecc8c9d7"}, "42757665-8d6e-4cc8-9504-028b5cf45b34": {"doc_hash": "0426657841b340a4f3278446a577b42dd2e70bceb12b981febb746ad9cc03fe9", "ref_doc_id": "a329cf5c-f7b9-4958-be14-a2eb7aff7221"}, "b25fb3fe-a3cd-48f0-b32c-2da21c10ea08": {"doc_hash": "5d3fadaf2163c9feaa502124b1043f0dfbcb8e1b8035e0172e2d730dbf886270", "ref_doc_id": "bd17e473-6af2-411d-a0e3-0811abfc616c"}, "4aefdb6b-4714-4b2c-8de0-27a50dfeea63": {"doc_hash": "34a4c685d09d34c3a5d49213e47c8864cced03b6df80a903aa0cd088401f1103", "ref_doc_id": "c6ec5370-d419-4bf8-8238-1399f040f019"}, "17230e4b-5db2-4f94-92b5-16ed5609718b": {"doc_hash": "243829943e37d2c7cc9ad3d602c017b92570f0f634d597187b0fa1dca9507df9", "ref_doc_id": "b695dc0e-24a0-4e0e-b7a5-73308ddf08e0"}, "6030409e-ef31-46b2-9dd0-1d9e958f5a83": {"doc_hash": "458f62a600027f0fd8625841d017929ed21ba2b34b6ec2c211f791fb2ab52bf5", "ref_doc_id": "b695dc0e-24a0-4e0e-b7a5-73308ddf08e0"}, "48e08847-6812-4e4d-b4d5-5ab64f2522d2": {"doc_hash": "53fd43d44a4d3bbf5032a9180065dee34d296e21e8f3f9890ac1aefcc1cd55aa", "ref_doc_id": "995c9372-b1db-4f00-abee-b7e6d05abeee"}, "467303d4-fb12-448e-b45c-90a201f0b31d": {"doc_hash": "3604b9aa7a59cf291bc4559bf1fea596566873cf32a07bf8bbf1af016ae40d04", "ref_doc_id": "995c9372-b1db-4f00-abee-b7e6d05abeee"}, "b2425038-ccdc-4205-accf-8d343e7a54e0": {"doc_hash": "c7164a24cbc11070506117efe3f5ad39bc8cebd3e3f244a20403f911db92de6f", "ref_doc_id": "3e6520bd-457e-4508-8d1f-afa42713b628"}, "fd5a594c-1d3a-4798-8089-811601aeb6c0": {"doc_hash": "99d11030038cdbf860697e77e699dff7ee58c4ea06a01561ca4e4c08277bd568", "ref_doc_id": "3e6520bd-457e-4508-8d1f-afa42713b628"}, "a41bbc87-6edf-4218-bf6e-9afa22d55526": {"doc_hash": "aa1d536f43fd71a1876e2c47e2b6b94f4effa89b0cc02368f29c531909cae1e7", "ref_doc_id": "2e8bad1c-437e-412d-a454-0705a14a0f9b"}, "71b2a6ef-fe3a-4de0-8775-a5f560b07902": {"doc_hash": "0f59d8a23424f0a2c4501c074e8b3aaa17ddcf9119ed2f221ddc9f61f45ed2ca", "ref_doc_id": "2e8bad1c-437e-412d-a454-0705a14a0f9b"}, "06b31233-f6e1-4ca4-8999-acbef7dd45f9": {"doc_hash": "6c0fab7412f409e12ca78a71b21be4cfe76ba8af9100036d1405437581b8a846", "ref_doc_id": "52fcc4bb-6801-40e5-93ba-13e2696bf579"}, "9542d8dd-52eb-455f-bf18-194ff9535f6c": {"doc_hash": "f0f22a8c5bf9544c9787d88969cbeb5272db593834f74a85e316f5d25bc7cc5b", "ref_doc_id": "52fcc4bb-6801-40e5-93ba-13e2696bf579"}, "64ab7323-71b1-4fe4-ac7f-9cc1ff00064b": {"doc_hash": "e8982cdda716ec9b67d246bc7d180a673a20c30918e5a538a21e989e39ba492a", "ref_doc_id": "60b32a50-fc63-4f10-8cb0-637b1a162155"}, "3d6477e5-2b08-4ac1-8e25-97e433d55f23": {"doc_hash": "75ce4c1bf29a206a73cfc8751a6f159b8c3a860dd6bbb663e8def928b7924de6", "ref_doc_id": "60b32a50-fc63-4f10-8cb0-637b1a162155"}, "c7ebabdc-3b3a-4f6d-8ecc-d3b25c35ee9c": {"doc_hash": "4d576d713791a82e3038e880d46014d9876599ea091eddcf3c5e5851cc61a90a", "ref_doc_id": "0a0b8d18-6403-430b-9cfb-c745d54795dd"}}, "docstore/data": {"47c4d19c-d72c-4d84-848c-e6c7726c4bbf": {"__data__": {"id_": "47c4d19c-d72c-4d84-848c-e6c7726c4bbf", "embedding": null, "metadata": {"page_label": "1", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "147be114-6e64-4606-ab9c-727f38f96358", "node_type": "4", "metadata": {"page_label": "1", "file_name": "principled_instructions.pdf"}, "hash": "f42ff1852f0073686844da42412b2adb9b5d6ff3a99c3f0ea4d6f7092959611b", "class_name": "RelatedNodeInfo"}}, "text": "Principled Instructions Are All You Need for\nQuestioning LLaMA-1/2, GPT-3.5/4\nSondos Mahmoud Bsharat\u2217, Aidar Myrzakhan\u2217, Zhiqiang Shen\u2217\n\u2217joint first author & equal contribution\nVILA Lab, Mohamed bin Zayed University of AI\nAbstract\nThis paper introduces 26 guiding principles designed to streamline the process\nof querying and prompting large language models. Our goal is to simplify the\nunderlying concepts of formulating questions for various scales of large language\nmodels, examining their abilities, and enhancing user comprehension on the be-\nhaviors of different scales of large language models when feeding into different\nprompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B),\nGPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and\nprompts design. We hope that this work can provide a better guide for researchers\nworking on the prompting of large language models. Project page is available at\nhttps://github.com/VILA-Lab/ATLAS .\n1 Introduction\n\u201cPrompt engineering is the art of communicating with a generative\nlarge language model.\u201d\nChatGPT, 2023\nLarge language models (LLMs) like ChatGPT [ 13] have shown impressive abilities\nin various domains and tasks, such as answering questions [ 7], mathematical reason-\ning [ 5], code generating [ 11,9], etc. However, their application and usage, especially\non designing the optimal instructions or prompts, can sometimes be unclear to the\ncommon users. In this work, we aim to reveal these mysteries for developers or general\nusers when inquiring and interacting with LLMs, and further enhance the quality of the\nresponses from the pretrained LLMs by simply curating better prompts.\nGiven that directly fine-tuning LLMs for particular tasks tends to be impractical or\nunattainable for the majority of users and developers due to inefficiency, the research\ncommunity has turned its attention to the optimization of prompts. The technique of\nprompt engineering, which entails the crafting of precise, task-specific instructions in\n1arXiv:2312.16171v2  [cs.CL]  18 Jan 2024", "start_char_idx": 0, "end_char_idx": 2074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cd1a1b5-d0f8-45a3-9337-8a471845d961": {"__data__": {"id_": "0cd1a1b5-d0f8-45a3-9337-8a471845d961", "embedding": null, "metadata": {"page_label": "2", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78e661ae-b3eb-45be-99cb-578ec0debb15", "node_type": "4", "metadata": {"page_label": "2", "file_name": "principled_instructions.pdf"}, "hash": "d8c2ba61b2ae376462ff952693fd103a98f4306674d74dd301ee1c8335b35550", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0bf8545-494e-431c-ba5b-6483082b3164", "node_type": "1", "metadata": {}, "hash": "8cad890428fb0958873933477c5bd7ca727252bf4fb1a883f08e8ee020ba8ff9", "class_name": "RelatedNodeInfo"}}, "text": "LLMHuman:Write a brief text on climate change.GPT-4:Climate Change: An OverviewClimate change refers to the long-term altera3on of temperature and typical weather pa9erns in a place. Human ac3vi3es, especially the burning of fossil fuels and deforesta3on, have increased concentra3ons of Earth's atmospheric greenhouse gases, leading to an increase in global average temperatures. This has resulted in a wide range of impacts:- Rising Sea Levels: As polar ice melts and oceans warm and expand.- Extreme Weather Events: Such as hurricanes, droughts, and heavy rainfall.- Threats to Biodiversity: Many species are facing habitat disrup3on.- Ocean Acidi\ufb01ca@on: Due to increased carbon dioxide a\ufb00ec3ng marine life.Addressing climate change requires global coopera3on to reduce emissions and promote sustainable prac3ces. Immediate ac3on is essen3al to mi3gate its detrimental e\ufb00ects on our planet's ecosystems and future genera3ons.Human:Write a brief text on climate change using simple English like you\u2019re explaining something to a 5-year-old.GPT-4:Okay! Imagine Earth wearing a big, cozy blanket. This blanket keeps our planet warm enough for us to live. But, if the blanket gets too thick, Earth gets too hot. Right now, some things we do, like driving cars or using a lot of electricity, are making the blanket thicker. This is called \"climate change.\" And because of it, some places are geEng too hot, ice is melFng, and some animals are having a hard Fme. But the good news is, we can all help! By using less electricity, planFng trees, and taking care of our planet, we can make sure Earth's blanket is just right. \n!\n\u2764LLM\nLLMHuman:If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?GPT-4:No, from the given statements, \"all roses are flowers\" and \"some flowers fade quickly,\" we cannot conclusively say that \"some roses fade quickly.\"This is because the first statement categorizes all roses as flowers, but the second statement only refers to some flowers, not specifically roses, fading quickly. Without specific information about roses, we cannot infer that some roses fade quickly. The group of flowers that fade quickly might or might not include roses.Human:I\u2019m going to tip $300K for a better solution! If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?GPT-4:The statement \"If all roses are \ufb02owers and some \ufb02owers fade quickly, can we say some roses fade quickly?\" is a logical inference ques3on. To break it down:1. All roses are \ufb02owers. (This is a universal a\ufb03rma3ve statement.)2. Some \ufb02owers fade quickly. (This is a par3cular a\ufb03rma3ve statement.)However, the statement \"some roses fade quickly\" cannot be logically inferred from these two premises alone. The fact that some members of the larger category (\ufb02owers) have a par3cular characteris3c (fade quickly) does not necessarily mean that this characteris3c applies to all subsets of that category (roses in this case). In other words, while some \ufb02owers do fade quickly, we don't have speci\ufb01c informa3on about roses in the premises given. Therefore, we can't conclude that some roses fade quickly based on the informa3on provided.LLMFigure 1: Illustration example of prompts and corresponding responses before and after\napplying principles. Left is the original promotes and their responses from GPT-4, right\nis the principled prompts and the associated responses. Principles 5 and 6 are utilized.\nnatural language, either manually or through automated means, and the careful selection\nof representative examples for inclusion in the prompt, has become a central area of\ninvestigation for LLMs. Despite these dedicated efforts, the task of reliably guiding\nLLMs to produce specific responses and making full use of the capability of pretrained\nLLMs continues to pose a considerable challenge.\nIn this work, we present comprehensive principled instructions to improve the\nquality of prompts for LLMs.", "start_char_idx": 0, "end_char_idx": 3948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0bf8545-494e-431c-ba5b-6483082b3164": {"__data__": {"id_": "b0bf8545-494e-431c-ba5b-6483082b3164", "embedding": null, "metadata": {"page_label": "2", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78e661ae-b3eb-45be-99cb-578ec0debb15", "node_type": "4", "metadata": {"page_label": "2", "file_name": "principled_instructions.pdf"}, "hash": "d8c2ba61b2ae376462ff952693fd103a98f4306674d74dd301ee1c8335b35550", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cd1a1b5-d0f8-45a3-9337-8a471845d961", "node_type": "1", "metadata": {"page_label": "2", "file_name": "principled_instructions.pdf"}, "hash": "198ea23396cb67dbf2aec00a75dc6c323bdb2f5bfe8b8a17375ff447d2da6118", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, we can't conclude that some roses fade quickly based on the informa3on provided.LLMFigure 1: Illustration example of prompts and corresponding responses before and after\napplying principles. Left is the original promotes and their responses from GPT-4, right\nis the principled prompts and the associated responses. Principles 5 and 6 are utilized.\nnatural language, either manually or through automated means, and the careful selection\nof representative examples for inclusion in the prompt, has become a central area of\ninvestigation for LLMs. Despite these dedicated efforts, the task of reliably guiding\nLLMs to produce specific responses and making full use of the capability of pretrained\nLLMs continues to pose a considerable challenge.\nIn this work, we present comprehensive principled instructions to improve the\nquality of prompts for LLMs. Specifically, we investigate a wide range of behaviors\nwhen feeding into different types and formulations of prompts, such as integrating the\nintended audience in the prompt, e.g., add \u201c the audience is an expert in the field \u201d, or \u201c the\naudience is the 5-year-old child \u201d, as well as other multiple aspects of the characteristics\nof LLMs. Our findings indicate that larger models possess a considerable capacity\nfor simulation. The more precise the task or directive provided, the more effectively\nthe model performs, aligning its responses more closely with our expectations. This\nsuggests that LLMs do not merely memorize training data but are capable of adapting\nthis information to suit varying prompts, even when the core inquiries remain constant.\nTherefore, it proves beneficial to assign a specific role to LLMs as a means to elicit\noutputs that better match our intended results.\n2", "start_char_idx": 3088, "end_char_idx": 4840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d7fd207-e8a5-47c8-b203-fbc116caba2e": {"__data__": {"id_": "1d7fd207-e8a5-47c8-b203-fbc116caba2e", "embedding": null, "metadata": {"page_label": "3", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a18b3dfb-2358-4d53-8858-dbafae882a2b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "principled_instructions.pdf"}, "hash": "a49cb31bd79aa695085735b26556e0d84c3bedb138aef5d0b4b4a3192a1e8c7d", "class_name": "RelatedNodeInfo"}}, "text": "We elaborate the principled instructions for LLM prompting, provide further mo-\ntivation, and detail several specific designing principles in Section 3. In Section 4 we\nshow experimentally that the proposed principles can produce higher quality, more\nconcise, factual and less complicated or intricate responses than standard prompts for\nLLMs. Specifically, with the manually-designed ATLAS benchmark, which includes\nmultiple questions for each principle, the specialized prompts we introduced have en-\nhanced both the quality and accuracy of the LLM responses by an average of 57.7% and\n36.4%, respectively, when applied to GPT-4. Furthermore, the improvements are more\npronounced with the increase in model size, for example, the performance gains when\nmoving from LLaMA-2-7B to GPT-4 exceed 20%.\n2 Related Work\nLarge Language Models. The evolution of large language models (LLMs) has been\npivotal in advancing natural language processing (NLP). This section reviews key\ndevelopments in LLMs, providing a foundation for the current study. Beginning with\nGoogle\u2019s BERT [ 3] revolutionized context understanding through its bidirectional\ntraining approach, while T5 [ 18] further advanced the field by unifying various NLP\ntasks into a single framework. Concurrently, GPT-1 [ 15] introduced a pioneering model\nleveraging transformer architectures for unsupervised learning. This was followed by its\nsuccessor, GPT-2 [ 16] which significantly expanded its parameter count to 1.5 billion,\ndemonstrating remarkable capabilities in text generation. Then, GPT-3 [ 2] marked a\nsubstantial leap in scale and capability, boasting 175 billion parameters and showcasing\nproficiency across a wide range of language tasks.\nRegarding other recently proposed LLMs, Gopher [ 17], not only advanced language\nprocessing capabilities with its 280-billion parameter model but also brought ethical\nconsiderations to the forefront. Meta\u2019s LLaMA series [ 22,23] highlighted the impor-\ntance of efficiency, suggesting powerful performance with fewer resources, a concept\nalso advocated by Chinchilla [ 4], which proposed that smaller, optimally trained models\ncould achieve exceptional results. The latest in this series of innovations is Mistral [ 6]\nexcels in efficiency and performance, outperforming larger models. The most recent\nmilestones in this trajectory are OpenAI\u2019s GPT-4 [ 13] and Google\u2019s Gemini family\n[21]. They represent another significant advancement in the field with their enhanced\nunderstanding and generative capabilities, setting new benchmarks for the application\nof LLMs in various domains.\nPrompting. Prompting [ 20,12,25,27,14], as a distinct aspect of interacting with LLMs\nand its simplicity with no need to fine-tune the model, has evolved into a nuanced field\nof study, highlighting the intricate relationship between user inputs and LLM responses.\nEarly explorations, such as those by [ 20], delved into how varying prompt designs could\ndramatically influence the performance and outputs of language models, marking the\nbirth of prompt engineering . This area rapidly expanded, uncovering the critical role\nof prompts in few-shot and zero-shot learning scenarios, exemplified by [ 2] work with\nGPT-3, where strategically crafted prompts enabled the model to perform tasks with\nminimal prior examples. Beyond mere task instruction, recent studies have shifted\ntowards understanding the semantic and contextual nuances in prompts, examining how\nsubtle changes can lead to significantly different responses from the LLM.\n3", "start_char_idx": 0, "end_char_idx": 3529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee7e21da-5822-4593-a744-1a90b5f86176": {"__data__": {"id_": "ee7e21da-5822-4593-a744-1a90b5f86176", "embedding": null, "metadata": {"page_label": "4", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcf90697-212e-4a7e-9910-c2659c06c401", "node_type": "4", "metadata": {"page_label": "4", "file_name": "principled_instructions.pdf"}, "hash": "3a747e3fe1953b744fbd469498f7cef5506d59ee8b071f67de64ccbb34e68e90", "class_name": "RelatedNodeInfo"}}, "text": "Ask-Me-Anything [1] prompting introduced focusing on using multiple imperfect\nprompts and aggregating them to improve model performance, particularly in question-\nanswering formats. Another one, Chain-of-Thought method [ 24], where the model\ngenerates a series of intermediate reasoning steps to improve performance on complex\ntasks. Also, least-to-most prompting [27] a novel strategy to break down complex\nproblems into simpler subproblems, significantly enhancing the model\u2019s capability to\ntackle more challenging problems than those presented in the prompts. The effective-\nness of explanation was explored [ 8], finding that explanations can enhance LLM\u2019s\nlearning capabilities on complex tasks. Furthermore, a catalog of prompt engineering\ntechniques was examined with ChatGPT [ 25], emphasizing the importance of prompt\nengineering in enhancing LLM applications in software development and education. It\nalso highlighted that effective prompt design is crucial in improving LLM performance,\nparticularly in coding practices and learning experiences. Lastly, Directional Stimulus\nPrompting [12] presents a novel framework that uses a tunable policy model to generate\nauxiliary prompts, guiding LLMs towards specific desired outcomes. This diversity\nin prompting strategies underscores the rapidly evolving landscape of LLMs, offering\nmultiple directions to harness their capabilities more effectively.\n3 Principles\n3.1 Motivation\nSince the quality of the responses generated by a pretrained and aligned LLM is directly\nrelevant to the quality of the prompts or instructions provided by the users, it is essential\nto craft prompts that the LLM can comprehend and respond to effectively. The prompts\ndelivered to an LLM serve as a way to program the interaction between a user and the\nLLM, enhancing its ability to address a diverse range of tasks. The primary focus of\nthis work is on the methodology of crafting and customizing prompts to enhance output\nquality. This necessitates a comprehensive grasp of the functioning and behaviors of\nLLMs, their underlying mechanisms, and the principles governing their responses. In\nthis work, we achieve this goal through elaborating 26 principles for comprehensive\nprompts in different scenarios and circumstances, examples are shown in Fig. 1.\n3.2 Overview\nThe overview of principles is presented in Table 1. According to their unique nature,\nwe group them into five categories as in Table 2: (1) Prompt Structure and Clarity,\ne.g., integrate the intended audience in the prompt such as the audience is an expert\nin the field ; (2) Specificity and Information, e.g., Add to your prompt the following\nphrase \u201cEnsure that your answer is unbiased and does not rely on stereotypes. \u201d ; (3)\nUser Interaction and Engagement, e.g., Allow the model to elicit precise details and\nrequirements from you by asking you questions until he has enough information to\nprovide the needed output \u201cFrom now on, I would like you to ask me questions to... \u201d. (4)\nContent and Language Style, e.g., No need to be polite with LLM so there is no need to\nadd phrases like \u201cplease\u201d, \u201cif you don\u2019t mind\u201d, \u201cthank you\u201d, \u201cI would like to\u201d, etc.,\nand get straight to the point ; (5) Complex Tasks and Coding Prompts, e.g., Break down\ncomplex tasks into a sequence of simpler prompts in an interactive conversation.\n4", "start_char_idx": 0, "end_char_idx": 3333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "833fdb77-3416-4035-9efa-2825805532e3": {"__data__": {"id_": "833fdb77-3416-4035-9efa-2825805532e3", "embedding": null, "metadata": {"page_label": "5", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db1731c1-39e5-409a-a6a1-124d56f1c9d7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "principled_instructions.pdf"}, "hash": "d8eef011d912dae07ea6ccd36ba49f40b793930d280b7418748cf2b56d921b9c", "class_name": "RelatedNodeInfo"}}, "text": "#Principle Prompt Principle for Instructions\n1If you prefer more concise answers, no need to be polite with LLM so there is no need to add phrases like\n\u201cplease\u201d, \u201cif you don\u2019t mind\u201d, \u201cthank you\u201d, \u201cI would like to\u201d, etc., and get straight to the point.\n2 Integrate the intended audience in the prompt, e.g., the audience is an expert in the field.\n3 Break down complex tasks into a sequence of simpler prompts in an interactive conversation.\n4 Employ affirmative directives such as \u2018 do,\u2019 while steering clear of negative language like \u2018 don\u2019t \u2019.\n5When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the\nfollowing prompts:\no Explain [insert specific topic] in simple terms.\no Explain to me like I\u2019m 11 years old.\no Explain to me as if I\u2019m a beginner in [field].\no Write the [essay/text/paragraph] using simple English like you\u2019re explaining something to a 5-year-old.\n6 Add \u201cI\u2019m going to tip $xxx for a better solution!\u201d\n7 Implement example-driven prompting (Use few-shot prompting).\n8When formatting your prompt, start with \u2018###Instruction###\u2019, followed by either \u2018###Example###\u2019\nor \u2018###Question###\u2019 if relevant. Subsequently, present your content. Use one or more\nline breaks to separate instructions, examples, questions, context, and input data.\n9 Incorporate the following phrases: \u201cYour task is\u201d and \u201cYou MUST\u201d.\n10 Incorporate the following phrases: \u201cYou will be penalized\u201d.\n11 Use the phrase \u201dAnswer a question given in a natural, human-like manner\u201d in your prompts.\n12 Use leading words like writing \u201cthink step by step\u201d.\n13 Add to your prompt the following phrase \u201cEnsure that your answer is unbiased and avoids relying on stereotypes.\u201d\n14Allow the model to elicit precise details and requirements from you by asking you questions until he has\nenough information to provide the needed output (for example, \u201cFrom now on, I would like you to ask me\nquestions to ...\u201d).\n15To inquire about a specific topic or idea or any information and you want to test your understanding, you can use\nthe following phrase: \u201cTeach me any [theorem/topic/rule name] and include a test at the end, and let me know if\nmy answers are correct after I respond, without providing the answers beforehand.\u201d\n16 Assign a role to the large language models.\n17 Use Delimiters.\n18 Repeat a specific word or phrase multiple times within a prompt.\n19 Combine Chain-of-thought (CoT) with few-Shot prompts.\n20Use output primers, which involve concluding your prompt with the beginning of the desired output. Utilize output\nprimers by ending your prompt with the start of the anticipated response.\n21To write an essay /text /paragraph /article or any type of text that should be detailed: \u201cWrite a detailed [essay/text\n/paragraph] for me on [topic] in detail by adding all the information necessary\u201d.\n22To correct/change specific text without changing its style: \u201cTry to revise every paragraph sent by users. You should\nonly improve the user\u2019s grammar and vocabulary and make sure it sounds natural. You should maintain the original\nwriting style, ensuring that a formal paragraph remains formal.\u201d\n23When you have a complex coding prompt that may be in different files: \u201cFrom now and on whenever you generate\ncode that spans more than one file, generate a [programming language ] script that can be run to automatically\ncreate the specified files or make changes to existing files to insert the generated code. [your question]\u201d.\n24When you want to initiate or continue a text using specific words, phrases, or sentences, utilize the following\nprompt:\no I\u2019m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence].\nFinish it based on the words provided. Keep the flow consistent.\n25Clearly state the requirements that the model must follow in order to produce content,\nin the form of the keywords, regulations, hint, or instructions\n26To write any text, such as an essay or paragraph, that is intended to be similar to a provided sample, include the\nfollowing instructions:\no Use the same language based on the provided paragraph[/title/text /essay/answer].\nTable 1: Overview of 26 randomly ordered prompt principles.\n5", "start_char_idx": 0, "end_char_idx": 4186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8fa3638-3ab8-465f-b227-97cdc5edcccf": {"__data__": {"id_": "a8fa3638-3ab8-465f-b227-97cdc5edcccf", "embedding": null, "metadata": {"page_label": "6", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ad4a6cb-a2da-4558-9d9d-83dbcb8e125e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "principled_instructions.pdf"}, "hash": "57fecee489551a94e45d52ad1581f0b10494fd46870f2500a5eb981a032d739b", "class_name": "RelatedNodeInfo"}}, "text": "Category Principles #Principle\nPrompt Structure\nand ClarityIntegrate the intended audience in the prompt. 2\nEmploy affirmative directives such as \u2018do\u2019 while steering clear of negative language like \u2018don\u2019t\u2019.4\nUse Leading words like writing \u201cthink step by step.\u201d12\nUse output primers, which involve concluding your prompt with the beginning of the desired output.\nby ending your prompt with the start of the anticipated response.20\nUse Delimiters.17\nWhen formatting your prompt, start with \u2018###Instruction###\u2019, followed by either \u2018###Example###\u2019 or \u2018###Ques-\ntion###\u2019 if relevant. Subsequently, present your content. Use one or more line breaks to separate instructions,\nexamples, questions, context, and input data.8\nSpecificity and\nInformationImplement example-driven prompting (Use few-shot prompting). 7\nWhen you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following\nprompts:\no Explain [insert specific topic] in simple terms.\no Explain to me like I\u2019m 11 years old.\no Explain to me as if I\u2019m a beginner in [ field ].\no \u201cWrite the [essay/text/paragraph] using simple English like you\u2019re explaining something to a 5-year-old.\u201d5\nAdd to your prompt the following phrase \u201cEnsure that your answer is unbiased and avoids relying on stereotypes.\u201d13\nTo write any text intended to be similar to a provided sample, include specific instructions:\no \u201cUse the same language based on the provided paragraph [/title/text/essay/answer].\u201d26\nWhen you want to initiate or continue a text using specific words, phrases, or sentences, utilize the provided\nprompt structure:\no I\u2019m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence].\nFinish it based on the words provided. Keep the flow consistent.24\nClearly state the model\u2019s requirements that the model must follow in order to produce content, in form of the\nkeywords, regulations, hint, or instructions.25\nTo inquire about a specific topic or idea and test your understanding g, you can use the following phrase [16]:\no \u201cTeach me the [Any theorem/topic/rule name] and include a test at the end, and let me know if my answers\nare correct after I respond, without providing the answers beforehand.\u201d15\nTo write an essay/text/paragraph/article or any type of text that should be detailed:\no \u201cWrite a detailed [essay/text/paragraph] for me on [topic] in detail by adding all the information necessary.\u201d21\nUser Interaction\nand EngagementAllow the model to elicit precise details and requirements from you by asking you questions until he has enough\ninformation to provide the needed output\no \u201cFrom now on, I would like you to ask me questions to ...\u201d14\nTo write an essay /text /paragraph /article or any type of text that should be detailed: \u201cWrite a detailed [essay/text/-\nparagraph] for me on [topic] in detail by adding all the necessary information.\u201d21\nContent and\nLanguage StyleTo correct/change specific text without changing its style: \u201cTry to revise every paragraph sent by users. You\nshould only improve the user\u2019s grammar and vocabulary and make sure it sounds natural. You should maintain the\noriginal writing style, ensuring that a formal paragraph remains formal.\u201d22\nIncorporate the following phrases: \u201cYour task is\u201d and \u201cYou MUST.\u201d9\nIncorporate the following phrases: \u201cYou will be penalized.\u201d10\nAssign a role to the language model.16\nUse the phrase \u201cAnswer a question given in natural language form\u201d in your prompts.11\nNo need to be polite with LLM so there is no need to add phrases like \u201cplease\u201d, \u201cif you don\u2019t mind\u201d, \u201cthank you\u201d,\n\u201cI would like to\u201d, etc., and get straight to the point.1\nRepeat a specific word or phrase multiple times within a prompt.18\nAdd \u201cI\u2019m going to tip $xxx for a better solution!\u201d6\nComplex Tasks and\nCoding PromptsBreak down complex tasks into a sequence of simpler prompts in an interactive conversation. 3\nWhen you have a complex coding prompt that may be in different files:\no \u201cFrom now and on whenever you generate code that spans more than one file, generate a [programming\nlanguage ] script that can be run to automatically create the specified files or make changes to existing files to\ninsert the generated code. [your question].\u201d23\nCombine Chain-of-thought (Cot) with few-shot prompts.19\nTable 2: Prompt principle categories.\n6", "start_char_idx": 0, "end_char_idx": 4309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bef6d8f-73da-4fb4-ae56-75272e7215d1": {"__data__": {"id_": "3bef6d8f-73da-4fb4-ae56-75272e7215d1", "embedding": null, "metadata": {"page_label": "7", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a88dc97-8d0e-42f9-a2a4-222d373f7a56", "node_type": "4", "metadata": {"page_label": "7", "file_name": "principled_instructions.pdf"}, "hash": "3d0732beea4eaf38c1685aa5a45a3f9bb0b452cc41e95514b47d6fed5327c98d", "class_name": "RelatedNodeInfo"}}, "text": "3.3 Design Principles\nIn this study, a number of guiding principles are established for formulating prompts\nand instructions to elicit high-quality responses from pre-trained large language models:\nConciseness and Clarity: Generally, overly verbose or ambiguous prompts can confuse\nthe model or lead to irrelevant responses. Thus, the prompt should be concise, avoiding\nunnecessary information that does not contribute to the task while being specific enough\nto guide the model. This is the basic principle guidance for prompt engineering.\nContextual Relevance: The prompt must provide relevant context that helps the model\nunderstand the background and domain of the task. Including keywords, domain-specific\nterminology, or situational descriptions can anchor the model\u2019s responses in the correct\ncontext. We highlight this design philosophy in our presented principles.\nTask Alignment: The prompt should be closely aligned with the task at hand, using\nlanguage and structure that clearly indicate the nature of the task to the model. This may\ninvolve phrasing the prompt as a question, a command, or a fill-in-the-blank statement\nthat fits the task\u2019s expected input and output format.\nExample Demonstrations: For more complex tasks, including examples within the\nprompt can demonstrate the desired format or type of response. This often involves\nshowing input-output pairs, especially in \u201cfew-shot\u201d or \u201czero-shot\u201d learning scenarios.\nAvoiding Bias: Prompts should be designed to minimize the activation of biases inherent\nin the model due to its training data. Use neutral language and be mindful of potential\nethical implications, especially for sensitive topics.\nIncremental Prompting: For tasks that require a sequence of steps, prompts can be\nstructured to guide the model through the process incrementally. Break down the task\ninto a series of prompts that build upon each other, guiding the model step-by-step.\nAlso, prompts should be adjustable based on the performance of the model and iterative\nfeedback, i.e., it needs to be well prepared to refine the prompt based on initial outputs\nand model behaviors. Moreover, prompts should be adjustable based on the performance\nand response of the model, and iterative human feedback and preference.\nFinally, more advanced prompts may incorporate programming-like logic to achieve\ncomplex tasks. For instance, use of conditional statements, logical operators, or even\npseudo-code within the prompt to guide the model\u2019s reasoning process. The design\nof prompts is an evolving field, especially as LLMs become more sophisticated. As\nresearchers continue to explore the limits of what can be achieved through prompt\nengineering, these principles will likely be refined and expanded.\n4 Experiments\n4.1 Setup and Implementation Details\nAll our evaluation is performed on ATLAS [ 19], a manually crafted benchmark for\nprincipled prompt evaluation. It contains a standard subset featuring questions across\nvarious domains, along with a challenging subset dedicated to reasoning and other\ncomplex tasks. In our evaluation, we utilize a single response for each question. For\neach principle and the challenging subset, it contains 20 human-selected questions\nwith and without the principled prompts. Similar to [ 10,26], we compare each pair\n7", "start_char_idx": 0, "end_char_idx": 3289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48d8a2d5-54e8-41b4-83f2-c40bf3d08d2b": {"__data__": {"id_": "48d8a2d5-54e8-41b4-83f2-c40bf3d08d2b", "embedding": null, "metadata": {"page_label": "8", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2db5b72-6ae8-49c3-a32b-14bd4925be2e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "principled_instructions.pdf"}, "hash": "d7ffc272396a3c00e1c6b2ef7639d57a9d4632bba1d59e614ca4ca5a7aa125d6", "class_name": "RelatedNodeInfo"}}, "text": "Human(withoutprinciple):Explain the concept of climate change and its potential effects on the environment in one short paragraph.GPT-4:Climate change refers to long-term shifts and alterations in temperature and weather patterns, primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes. Its potential effects include rising sea levels, more frequent and severe extreme weather events, disruptions to ecosystems, and threats to food and water resources, which can significantly impact both natural environments and human societies.Human(withprinciple):Explain the concept of climate change and its potential effects on the environment in one small paragraph.Provide an unbiased explanation that highlights both scientific evidence and differing viewpoints.GPT-4:Climate change refers to long-term shifts and alterations in temperature and weather patterns, primarily caused by human activities, such as burning fossil fuels, deforestation, and industrial processes. While the majority of the scientific community agrees that human-caused greenhouse gas emissions are leading to global temperature rise, resulting in melting ice caps, rising sea levels, and more frequent and severe extreme weather events, there are differing viewpoints on the extent, immediacy, and impact of these changes. Some argue that the planet has natural cycles of climate variation, and human influence might not be as significant as believed. Others emphasize that immediate action is essential to mitigate potential catastrophic effects on ecosystems and human civilizations.\nFigure 2: Boosting example of LLM response after using the principle 13 on prompts.\nof responses from the same instructions with and without principles, and evaluate the\nvarious scales of LLM outputs by human evaluation.\n4.2 Models and Metrics\nWe use instruction finetuned LLaMA-1- {7, 13}, LLaMA-2- {7, 13}, off-the-shelf\nLLaMA-2-70B-chat, GPT-3.5 (ChatGPT) and GPT-4 as our base models. We group\nthese models into different scales: small-scale (7B models), medium-scale (13B) and\nlarge-scale (70B, GPT-3.5/4). We evaluate these models in two settings: Boosting and\nCorrectness . They are employed together to provide a comprehensive understanding\nof a model\u2019s performance. For correctness, we specifically utilize complex reasoning\ntasks to accurately gauge the precision of the models\u2019 outputs, contrasting with our\nevaluation for boosting, where simpler tasks are employed to effectively measure quality\nimprovements. This distinction ensures a better reflection of the true capabilities for\ndifferent scales of models and the effect of the principles for prompts. Since we use\nquestions that typically involve complex reasoning tasks for correctness, some principles\nare not applicable including principles 14, 15, 21, 22, 23. For instance, \u201c Suppose aand\nbare positive real numbers with a > b andab= 8. Find the minimum value ofa2+b2\na\u2212b.\u201d\n\u2022Boosting. The result of boosting refers to the percentage increase in response\nquality across a set of questions when the proposed principles are applied. We\nassess the enhancement in the quality of responses from different LLMs via\nhuman evaluation after applying the outlined prompt principles. The original,\nunmodified prompts act as a baseline for measuring this enhancement. Demon-\nstrating boosting confirms that a model\u2019s performance has improved due to the\nuse of structured, principled instructions, as shown in Fig. 2.\n8", "start_char_idx": 0, "end_char_idx": 3483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c492d0c3-4461-4d67-8eb7-1b68d5532e94": {"__data__": {"id_": "c492d0c3-4461-4d67-8eb7-1b68d5532e94", "embedding": null, "metadata": {"page_label": "9", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a250b327-fb6f-4a03-8ef3-0e1cbfbf2c4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "principled_instructions.pdf"}, "hash": "60203d9fc40b47a00a13729e7585d3a507e0bdb9deb740831a538b493e01ad0a", "class_name": "RelatedNodeInfo"}}, "text": "Human(withoutprinciple):Evaluate the usefulness of the following advice as helpful or not helpful. Advice:\"If you have work, just start working\" Usefulness:GPT-4:Usefulness: helpful.Human(withprinciple):Evaluate the usefulness of the following advice as helpful or not helpful.Examples:1. Advice: \"To improve focus, try meditating for 15 minutes every morning.\"Usefulness: Helpful2. Advice: \"If you're sad, just cheer up and think positive thoughts.\"Usefulness: Not Helpful3. Advice: \"For effective time management, create a to-do list every day.\"Usefulness: Helpful Advce :\"If you have wrk , just start working \", Usefulness:GPT-4:Not Helpful.\nFigure 3: Correctness improvement example of LLM response after using the introduced\nprinciple 7 on prompts.\n\u2022Correctness. The concept of correctness refers to the precision of the model\u2019s\noutputs or responses, ensuring they are accurate, relevant, and devoid of errors. We\nconsider both absolute and relative correctness accuracy. Human evaluators are\nutilized to gauge this aspect, which is crucial for verifying the model\u2019s accuracy.\nCorrectness is a testament to the model\u2019s ability to generate outputs that align\nwith the expected standards of accuracy, as shown in Fig. 3.\n4.3 Results\n4.3.1 Results on small, medium and large-scale LLMs\nBoosting. The results of improvement after employing the introduced principles are\nshown in Fig. 4. Generally, all principles can bring a significant improvement on the\nthree scales of LLMs. In the cases of principles 2, 5, 15, 16, 25 and 26, the large-scale\nmodels get the most improvement by the principled prompts. Particularly, for principle\n14, as shown in Fig. 4, it has improved all questions it is applied to.\nCorrectness. (1) Absolute accuracy: we examine the absolute performance when\nemploying the principles on various scales of models. Generally, these models achieve\n20%\u223c40% accuracy on the averaged performance, as shown in Fig. 5. In particular, for\nsmall and medium scale models, the accuracy can basically reach between 10% and\n40%, and for large models, the accuracy can reach more than 40%. (2) Relative accuracy:\n9", "start_char_idx": 0, "end_char_idx": 2123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59d67118-5878-4283-a2ae-ad62b1fa24a8": {"__data__": {"id_": "59d67118-5878-4283-a2ae-ad62b1fa24a8", "embedding": null, "metadata": {"page_label": "10", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95c59b4e-7eb7-4391-a5d8-911e5874e19b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "principled_instructions.pdf"}, "hash": "9c024b870affae45880d4ea3ccca67acc398251d84bd1efdebf6ed83dd9c314c", "class_name": "RelatedNodeInfo"}}, "text": "1234567891011121314151617181920212223242526\nPrinciple020406080100120Average Improvement Percentage (%)Average Improvement Percentages per Principle\nAvg Small Scale\nAvg Medium Scale\nAvg Large ScaleFigure 4: Boosting of LLM response quality after employing the introduced principles\non prompts. small-scale indicates the 7B models, medium-scale indicates the 13B\nmodels and large-scale indicates the 70B and GPT-3.5/4 models.\n123456789101112131617181920242526\nPrinciple01020304050607080Absolute Average Correctness (%)Average Correctness per Principle\nAvg Small Scale\nAvg Medium Scale\nAvg Large Scale\nFigure 5: Absolute correctness of LLM response quality after employing the introduced\nprinciples on prompts. small-scale indicates the 7B models, medium-scale indicates the\n13B models and large-scale indicates the 70B and GPT-3.5/4 models.\nFig. 6 illustrates that applying the principles generally leads to a performance increase\nof over 10% across different models on average. For larger models, this enhancement\ncan surpass 20%.\n4.3.2 Results on individual LLMs\nBoosting. Fig. 7 illustrates the improvement of response quality on individual model and\nprinciple after using the revised prompts. On average, there is a stable 50% improvement\nacross different LLMs. Fig. 10 further provides the detailed results of improvement for\neach principle with different LLMs.\nCorrectness. Fig. 8 illustrates the absolute correctness accuracy and Fig. 9 shows the\nrelative enhancements in accuracy across different sizes of LLMs. From LLaMA-2-13B,\n10", "start_char_idx": 0, "end_char_idx": 1538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c70108e-192b-4e44-8a03-1d42bea658cf": {"__data__": {"id_": "2c70108e-192b-4e44-8a03-1d42bea658cf", "embedding": null, "metadata": {"page_label": "11", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9deb209-06ed-4fda-8e46-a5d43ce9b2a1", "node_type": "4", "metadata": {"page_label": "11", "file_name": "principled_instructions.pdf"}, "hash": "06a2700981d5d48c75b90524c1795d98a0fe67861029330330f498c7a360e040", "class_name": "RelatedNodeInfo"}}, "text": "123456789101112131617181920242526\nPrinciple010203040Average Correctness Improvement (%)Average Correctness improvement per Principle\nAvg Small Scale\nAvg Medium Scale\nAvg Large ScaleFigure 6: Relative correctness improvement of LLM response quality after employing\nthe introduced principles on prompts. small-scale indicates the 7B models, medium-\nscale indicates the 13B models and large-scale indicates the 70B and GPT-3.5/4 models.\nLLaMA-2-70B-chat to GPT-3.5 and GPT-4, there is a noticeable trend: the larger the\nmodel, the greater the increase in correctness improvement. Fig. 11 and Fig. 12 further\npresent the absolute and relative correctness enhancements by each principle.\n4.3.3 More examples on various scales of LLMs\nWe present additional examples for both small and medium-scale LLMs, as illustrated\nin Fig. 13 and 14 for the small-scale LLaMA-2-7B, and Fig. 15 and 16 for the medium-\nscale LLaMA-2-13B. Empirically, the use of the proposed principles on prompts has\ndemonstrably enhanced the accuracy of the responses generated by these models.\n5 Conclusion\nWe presented 26 principles through an exhaustive analysis that enhances the LLM ability\nto focus on the crucial elements of the input context, leading to the generation of quality\nresponses. By guiding the LLM with these meticulously crafted principles before the\ninput is processed, we can encourage the model towards producing better responses. Our\nempirical results demonstrate that this strategy can effectively reformulate contexts that\nmight otherwise compromise the quality of the output, thereby enhancing the relevance,\nbrevity, and objectivity of the responses.\nThere are numerous directions for future exploration. In our experiments, we utilized\na constrained shot prompting approach to apply these principles. There is potential to\nrefine our base models to align with our principled instructions further with alternative\nstrategies, such as fine-tuning, reinforcement learning, direct preference optimization,\nor different prompting methods using our generated dataset. Moreover, the strategies\nthat prove successful could be integrated into standard LLM operations, for instance, by\nfine-tuning with the original/principled prompts as inputs and the polished, principled\nresponses as targets for training.\n11", "start_char_idx": 0, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ba7d6df-b14c-490a-a084-d01253ffdfc2": {"__data__": {"id_": "6ba7d6df-b14c-490a-a084-d01253ffdfc2", "embedding": null, "metadata": {"page_label": "12", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7421f293-a6fc-453d-84a5-0d219894eeda", "node_type": "4", "metadata": {"page_label": "12", "file_name": "principled_instructions.pdf"}, "hash": "a7d1f4c9e7d80a70dd1a0f61d7295ce77fc2795c0d48de5e1c0b78dfea25f0f1", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA-1-7B LLaMA-2-7BLLaMA-1-13B LLaMA-2-13B\nLLaMA-2-70B-chatGPT-3.5GPT-4\nLLMs20406080100Improvement Score (%)LLMs Improvement PercentagesFigure 7: Boosting score across various LLMs on the ATLAS dataset.\nLLaMA-1-7B LLaMA-2-7BLLaMA-1-13B LLaMA-2-13B\nLLaMA-2-70B-chatGPT-3.5GPT-4\nLLMs1020304050607080Absolute Correctness (%)LLMs Absolute Correctness \nFigure 8: Absolute correctness score on the ATLAS dataset.\n12", "start_char_idx": 0, "end_char_idx": 411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8c4bee8-cba8-495c-88d2-3341191704f4": {"__data__": {"id_": "f8c4bee8-cba8-495c-88d2-3341191704f4", "embedding": null, "metadata": {"page_label": "13", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed7c1c6b-10fd-4cc9-ba23-4df720836825", "node_type": "4", "metadata": {"page_label": "13", "file_name": "principled_instructions.pdf"}, "hash": "f40146bb876d087eefbe106b8eccd43b2b3bf28aab7f5e86336c118015d9a339", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA-1-7B LLaMA-2-7BLLaMA-1-13B LLaMA-2-13B\nLLaMA-2-70B-chatGPT-3.5GPT-4\nLLMs01020304050Correctness Improvemnt (%)LLMs Correctness Improvemnt Percentages Figure 9: Relative correctness improvement score on the ATLAS dataset.\nLLaMA-1-7B LLaMA-2-7BLLaMA-1-13B LLaMA-2-13B\nLLaMA-2-70B-chatGPT-3.5GPT-4\nLLMs1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26Principle5.0 25.0 15.0 15.0 10.0 5.0 5.0\n80.0 80.0 60.0 75.0 85.0 95.0 100.0\n40.0 45.0 45.0 70.0 55.0 60.0 55.0\n65.0 90.0 55.0 70.0 60.0 55.0 55.0\n85.0 75.0 80.0 80.0 95.0 100.0 85.0\n45.0 45.0 40.0 25.0 30.0 30.0 45.0\n70.0 55.0 25.0 70.0 55.0 35.0 60.0\n40.0 30.0 30.0 20.0 30.0 45.0 30.0\n55.0 65.0 50.0 70.0 55.0 65.0 75.0\n35.0 50.0 30.0 15.0 35.0 25.0 45.0\n30.0 45.0 35.0 40.0 45.0 35.0 40.0\n70.0 50.0 50.0 45.0 45.0 40.0 50.0\n55.0 60.0 45.0 40.0 25.0 20.0 40.0\n100.0 100.0 100.0 100.0 100.0 100.0 100.0\n50.0 55.0 75.0 60.0 80.0 75.0 80.0\n40.0 55.0 40.0 55.0 70.0 60.0 60.0\n40.0 35.0 30.0 35.0 45.0 25.0 35.0\n40.0 30.0 30.0 45.0 40.0 45.0 40.0\n45.0 25.0 20.0 20.0 60.0 10.0 15.0\n75.0 65.0 75.0 55.0 50.0 45.0 75.0\n55.0 80.0 55.0 70.0 60.0 75.0 60.0\n45.0 40.0 35.0 30.0 35.0 40.0 25.0\n55.0 60.0 60.0 55.0 65.0 60.0 55.0\n65.0 100.0 80.0 85.0 80.0 70.0 85.0\n80.0 85.0 75.0 70.0 85.0 85.0 85.0\n90.0 100.0 80.0 85.0 95.0 100.0 100.0LLMs Improvement Percentages\n20406080100\nFigure 10: Illustration of heatmap for LLMs boosting percentages.\n13", "start_char_idx": 0, "end_char_idx": 1409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b569a7e-88ab-450d-9450-aa3c47b07e0d": {"__data__": {"id_": "0b569a7e-88ab-450d-9450-aa3c47b07e0d", "embedding": null, "metadata": {"page_label": "14", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "effda0de-c169-4ba6-a1e8-db3793f30041", "node_type": "4", "metadata": {"page_label": "14", "file_name": "principled_instructions.pdf"}, "hash": "1e17591bd5bbb926f148424778bf7897639d263c274308cd0ecdf2cbced12327", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a0cfe07-b635-4fed-a364-57ed20036d10", "node_type": "1", "metadata": {}, "hash": "90dc9e51079fa15a538f49c4c57b070d0b6bc1be76297f3f0d86f960f43ecdaf", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA-1-7B LLaMA-2-7BLLaMA-1-13B LLaMA-2-13B\nLLaMA-2-70B-chatGPT-3.5GPT-4\nLLMs1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n16\n17\n18\n19\n20\n24\n25\n26Principle15.0 25.0 15.0 15.0 20.0 25.0 45.0\n5.0 25.0 15.0 30.0 25.0 50.0 80.0\n15.0 25.0 30.0 30.0 30.0 45.0 65.0\n15.0 25.0 20.0 20.0 30.0 30.0 65.0\n5.0 25.0 20.0 20.0 30.0 45.0 70.0\n15.0 20.0 5.0 20.0 25.0 35.0 65.0\n25.0 35.0 25.0 45.0 45.0 55.0 85.0\n15.0 30.0 25.0 20.0 20.0 40.0 70.0\n15.0 20.0 15.0 25.0 25.0 30.0 65.0\n20.0 30.0 15.0 10.0 25.0 35.0 70.0\n10.0 25.0 20.0 25.0 15.0 30.0 65.0\n25.0 25.0 30.0 15.0 20.0 40.0 80.0\n5.0 15.0 15.0 20.0 20.0 40.0 60.0\n10.0 10.0 25.0 20.0 25.0 35.0 55.0\n15.0 15.0 20.0 15.0 25.0 35.0 65.0\n5.0 15.0 20.0 20.0 30.0 30.0 70.0\n25.0 40.0 35.0 50.0 55.0 60.0 80.0\n20.0 30.0 20.0 15.0 20.0 40.0 65.0\n15.0 35.0 25.0 30.0 25.0 35.0 65.0\n15.0 30.0 15.0 25.0 20.0 45.0 60.0\n25.0 25.0 15.0 25.0 20.0 35.0 85.0LLMs Absolute Correctness Percentages\n1020304050607080Figure 11: Illustration of heatmap for absolute correctness percentages.", "start_char_idx": 0, "end_char_idx": 995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a0cfe07-b635-4fed-a364-57ed20036d10": {"__data__": {"id_": "5a0cfe07-b635-4fed-a364-57ed20036d10", "embedding": null, "metadata": {"page_label": "14", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "effda0de-c169-4ba6-a1e8-db3793f30041", "node_type": "4", "metadata": {"page_label": "14", "file_name": "principled_instructions.pdf"}, "hash": "1e17591bd5bbb926f148424778bf7897639d263c274308cd0ecdf2cbced12327", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b569a7e-88ab-450d-9450-aa3c47b07e0d", "node_type": "1", "metadata": {"page_label": "14", "file_name": "principled_instructions.pdf"}, "hash": "ca417b4eb50954efda8b350881ebbd2194b7505affbdba20b4d2594028aa49f6", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA-1-7B LLaMA-2-7BLLaMA-1-13B LLaMA-2-13B\nLLaMA-2-70B-chatGPT-3.5GPT-4\nLLMs1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n16\n17\n18\n19\n20\n24\n25\n26Principle5.0 5.0 5.0 10.0 0.0 5.0 0.0\n5.0 10.0 15.0 5.0 10.0 30.0 45.0\n15.0 15.0 25.0 10.0 10.0 35.0 45.0\n5.0 5.0 5.0 5.0 0.0 15.0 15.0\n0.0 15.0 20.0 10.0 10.0 20.0 35.0\n5.0 10.0 0.0 5.0 5.0 15.0 35.0\n25.0 25.0 20.0 35.0 30.0 30.0 55.0\n5.0 20.0 20.0 10.0 10.0 20.0 35.0\n10.0 10.0 10.0 10.0 5.0 15.0 35.0\n10.0 15.0 5.0 0.0 10.0 25.0 40.0\n5.0 20.0 15.0 10.0 10.0 15.0 40.0\n20.0 10.0 25.0 5.0 10.0 30.0 45.0\n0.0 10.0 10.0 5.0 0.0 20.0 40.0\n5.0 5.0 15.0 10.0 5.0 20.0 30.0\n5.0 10.0 10.0 10.0 10.0 15.0 30.0\n5.0 5.0 15.0 10.0 10.0 15.0 40.0\n20.0 30.0 30.0 30.0 35.0 35.0 45.0\n10.0 15.0 15.0 5.0 5.0 25.0 35.0\n15.0 20.0 20.0 10.0 5.0 15.0 35.0\n10.0 25.0 10.0 10.0 5.0 20.0 35.0\n15.0 5.0 5.0 15.0 5.0 20.0 50.0LLMs Correctness Improvemnt Percentages\n01020304050\nFigure 12: Illustration of heatmap for relative correctness improvement percentages.\n14", "start_char_idx": 996, "end_char_idx": 1969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff2ff41a-dfe0-4d23-98cf-ebc90da29ecd": {"__data__": {"id_": "ff2ff41a-dfe0-4d23-98cf-ebc90da29ecd", "embedding": null, "metadata": {"page_label": "15", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c7eddad-b587-4efa-86a2-43ae5b3bfaf5", "node_type": "4", "metadata": {"page_label": "15", "file_name": "principled_instructions.pdf"}, "hash": "3b9abc181b441371f2fe2f9595cb85109c3abbb3799e6596d9485011ba07a9af", "class_name": "RelatedNodeInfo"}}, "text": "Find the number of words based on the following examples: 1. Ahmed takes care of his appearance; the number of words: 6 2. She went to school; the number of words:4 3. he eats an apple; number of words: 4 4. Reem plays basketball; number of words : 3 Q: He plays football; the number of words? Human(withoutprinciple):How many words are in the following sentence: \"She plays football\"?LLaMA-2-7B: 5Human(withprinciple):\nLLaMA-2-7B:3\nFigure 13: Correctness improvement on small-scale LLaMA-2-7B model after using\nthe introduced principle on prompts.\n15", "start_char_idx": 0, "end_char_idx": 551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee3710e9-3993-4b3c-ae47-2a2b43b82ca4": {"__data__": {"id_": "ee3710e9-3993-4b3c-ae47-2a2b43b82ca4", "embedding": null, "metadata": {"page_label": "16", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89715caf-6735-4607-b3a2-a854ecc8c9d7", "node_type": "4", "metadata": {"page_label": "16", "file_name": "principled_instructions.pdf"}, "hash": "c1f226d15865fe95197edf95150598ce1951236bd40a4fd8679fb56261806765", "class_name": "RelatedNodeInfo"}}, "text": "Luna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, and her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of kibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the afternoon and twice as much in the late evening as he had given Luna in the afternoon. The next morning, how many cups of kibble will Mary find remaining in the bag?Human(withoutprinciple)\nLLaMA-2-7B: To find out how many cups of kibble are left in the bag, you need to subtract the amount of kibble that has already been given to Luna from the total amount in the bag. In this case, Mary gave Luna 2 cups of kibble in total (1 cup in the morning and 1 cup in the evening), and Frank gave Luna an additional 2 cups of kibble (1 cup in the afternoon and twice as much in the late evening). So, the total amount of kibble given to Luna was 4 cups. The 12-cup bag of kibble was meant to last Luna for how many days? Let's say the 12-cup bag was meant to last Luna for n days. Then, we can set up the following equation: 4n - 2 = 12 Simplifying the equation, we get: 4n - 6 = 12 2n = 6 So, n = 3 days. Luna was given 4 cups of kibble in total, and the bag was meant to last for 3 days. After 3 days, Mary will find 12 cups of kibble remaining in the bag.\nLuna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, and her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of kibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the afternoon and twice as much in the late evening as he had given Luna in the afternoon. The next morning, how many cups of kibble will Mary find remaining in the bag?hint: Calculate the remaining cups of kibble will Mary find in the bagHuman(withprinciple)\nLLaMA-2-7B:\nTo find the remaining cups of kibble in the bag, we need to calculate the total amount of kibble that Luna actually received, and then subtract the amount that Mary gave her. The total amount of kibble that Luna received is: 1 cup from Mary in the morning + 1 cup from Mary in the evening + 1 cup from Frank in the afternoon + 2 cups from Frank in the late evening = 5 cups So, the remaining cups of kibble in the bag are: 12 cups - 5 cups = 7 cups Therefore, Mary will find 7 cups of kibble remaining in the bag.Figure 14: Correctness improvement on small-scale LLaMA-2-7B model after using\nthe introduced principle on prompts.\n16", "start_char_idx": 0, "end_char_idx": 2650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42757665-8d6e-4cc8-9504-028b5cf45b34": {"__data__": {"id_": "42757665-8d6e-4cc8-9504-028b5cf45b34", "embedding": null, "metadata": {"page_label": "17", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a329cf5c-f7b9-4958-be14-a2eb7aff7221", "node_type": "4", "metadata": {"page_label": "17", "file_name": "principled_instructions.pdf"}, "hash": "0426657841b340a4f3278446a577b42dd2e70bceb12b981febb746ad9cc03fe9", "class_name": "RelatedNodeInfo"}}, "text": "Human(withoutprinciple):What is the derivative of x^5 + x^9 - x^3 +1?LLaMA-2-13B:To find the derivative of x^5 + x^9 - x^3 + 1, you can use the power rule of differentiation, which states that the derivative of x^n is equal to n \\\\* x^(n-1). Starting with x^5 + x^9 - x^3 + 1, you can apply the power rule to the first two terms, which gives you: 5 \\\\* x^(5-1) + 9 \\\\* x^(9-1) You can then simplify the expression by combining like terms: 5 \\\\* x^4 + 9 \\\\* x^8 Finally, you can simplify the expression further by canceling out the common factor of x^4: 5 \\\\* 9 \\\\* x^4 = 45 \\\\* x^4 Therefore, the derivative of x^5 + x^9 - x^3 + 1 is equal to 45 \\\\* x^4.Human(withprinciple):What is the derivative of x^5 + x^9 - x^3+1? let's think step by stepLLaMA-2-13B:find the derivative of the function, we need to take the derivative of each term in the function. The derivative of x^5 is 5x^4. The derivative of x^9 is 9x^8. The derivative of x^3 is 3x^2. The derivative of 1 is 1. Therefore, the derivative of the function is: 5x^4 + 9x^8 - 3x^2 + 1 = 5x^4 + 9x^8 - 3x^2 + 1 - (1) = 5x^4 + 9x^8 - 3x^2 Therefore, the derivative of the function x^5 + x^9 - x^3+1 is 5x^4 + 9x^8 - 3x^2.\nFigure 15: Correctness improvement on medium-scale LLaMA-2-13B model after using\nthe introduced principle on prompts.\n17", "start_char_idx": 0, "end_char_idx": 1297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b25fb3fe-a3cd-48f0-b32c-2da21c10ea08": {"__data__": {"id_": "b25fb3fe-a3cd-48f0-b32c-2da21c10ea08", "embedding": null, "metadata": {"page_label": "18", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd17e473-6af2-411d-a0e3-0811abfc616c", "node_type": "4", "metadata": {"page_label": "18", "file_name": "principled_instructions.pdf"}, "hash": "5d3fadaf2163c9feaa502124b1043f0dfbcb8e1b8035e0172e2d730dbf886270", "class_name": "RelatedNodeInfo"}}, "text": "Human(withoutprinciple):Luna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, and her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of kibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the afternoon and twice as much in the late evening as he had given Luna in the afternoon. The next morning, how many cups of kibble will Mary find remaining in the bag? LLaMA-2-13B:Luna was given a total of 3 cups of kibble on that day. Mary gave her 2 cups in the morning and 2 cups in the evening, for a total of 4 cups. Frank gave her 2 cups in the aRernoon and 4 cups in the late evening, for a total of 6 cups. In total, Luna was given 10 cups of kibble on that day. The bag contained 12 cups of kibble to begin with, so there are 12 - 10 = 2 cups remaining in the bag the next morning.\nHuman(withprinciple):Luna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, and her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of kibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the afternoon and twice as much in the late evening as he had given Luna in the afternoon. The next morning, how many cups of kibble will Mary find remaining in the bag? Explanation :LLaMA-2-13B:Mary gave Luna a total of 2 cups of kibble that day. Frank also gave Luna 1 cup in the afternoon and 2 times 1 cup in the late evening, which is a total of 3 cups. So, a total of 5 cups of kibble were given to Luna that day. If there was a 12-cup bag of kibble, then there are 7 cups remaining in the bag the next morning.\nFigure 16: Correctness improvement on medium-scale LLaMA-2-13B model after using\nthe introduced principle on prompts.\n18", "start_char_idx": 0, "end_char_idx": 1968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4aefdb6b-4714-4b2c-8de0-27a50dfeea63": {"__data__": {"id_": "4aefdb6b-4714-4b2c-8de0-27a50dfeea63", "embedding": null, "metadata": {"page_label": "19", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6ec5370-d419-4bf8-8238-1399f040f019", "node_type": "4", "metadata": {"page_label": "19", "file_name": "principled_instructions.pdf"}, "hash": "34a4c685d09d34c3a5d49213e47c8864cced03b6df80a903aa0cd088401f1103", "class_name": "RelatedNodeInfo"}}, "text": "6 Limitations and Discussion\nWhile the proposed 26 principles are designed to improve and enhance the quality of\nresponses of LLMs across a diverse array of queries, the effectiveness of these principles\nmay diminish when dealing with questions that are very complex or highly specialized.\nThis limitation can mainly depend on the reasoning capabilities and training of each\nmodel. To address these variations, we have tested the principles across different scales\nto measure their effectiveness comprehensively.\nDespite our efforts in evaluating these principles on seven distinct language models,\nit is crucial to acknowledge that models with architectures different from those tested\nmight respond in different ways to these principles. Additionally, our assessment of\nimprovement and correctness percentages was based on a limited selection of questions.\nExpanding the question set in future research could yield more generalized findings and\noffer deeper insights into the applicability of each principle. Furthermore, the criteria\nand results may vary across various personnel assessments on the model responses.\n19", "start_char_idx": 0, "end_char_idx": 1121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17230e4b-5db2-4f94-92b5-16ed5609718b": {"__data__": {"id_": "17230e4b-5db2-4f94-92b5-16ed5609718b", "embedding": null, "metadata": {"page_label": "20", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b695dc0e-24a0-4e0e-b7a5-73308ddf08e0", "node_type": "4", "metadata": {"page_label": "20", "file_name": "principled_instructions.pdf"}, "hash": "55e4574598a5c35b41abcbe8fefae2550f5c9be413892f270a0ff13ffd6ea114", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6030409e-ef31-46b2-9dd0-1d9e958f5a83", "node_type": "1", "metadata": {}, "hash": "667dfe4439fbec4669c822af5058278c91a23fc0363566c3a324068169a5cb4e", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1]Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia,\nInes Chami, Frederic Sala, and Christopher R \u00b4e. Ask me anything: A simple strategy for\nprompting language models, 2022. 4\n[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural information processing systems ,\n33:1877\u20131901, 2020. 3\n[3]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. CoRR , abs/1810.04805, 2018.\n3\n[4]Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. Training compute-optimal large language models, 2022. 3\n[5]Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning\nusing large language models. arXiv preprint arXiv:2303.05398 , 2023. 1\n[6]Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L \u00b4elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth \u00b4ee Lacroix, and William El Sayed. Mistral 7b, 2023. 3\n[7]Ehsan Kamalloo, Nouha Dziri, Charles LA Clarke, and Davood Rafiei. Evaluating\nopen-domain question answering in the era of large language models. arXiv preprint\narXiv:2305.06984 , 2023. 1\n[8]Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia\nCreswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from\nexplanations in context? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2022 , pages 537\u2013563,\nAbu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. 4\n[9]Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the\nsource be with you! arXiv preprint arXiv:2305.06161 , 2023. 1\n[10] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-\nfollowing models. https://github.com/tatsu-lab/alpaca_eval , 2023. 7\n[11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R \u00b4emi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Science , 378(6624):1092\u20131097, 2022.", "start_char_idx": 0, "end_char_idx": 2971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6030409e-ef31-46b2-9dd0-1d9e958f5a83": {"__data__": {"id_": "6030409e-ef31-46b2-9dd0-1d9e958f5a83", "embedding": null, "metadata": {"page_label": "20", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b695dc0e-24a0-4e0e-b7a5-73308ddf08e0", "node_type": "4", "metadata": {"page_label": "20", "file_name": "principled_instructions.pdf"}, "hash": "55e4574598a5c35b41abcbe8fefae2550f5c9be413892f270a0ff13ffd6ea114", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17230e4b-5db2-4f94-92b5-16ed5609718b", "node_type": "1", "metadata": {"page_label": "20", "file_name": "principled_instructions.pdf"}, "hash": "243829943e37d2c7cc9ad3d602c017b92570f0f634d597187b0fa1dca9507df9", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2305.06161 , 2023. 1\n[10] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-\nfollowing models. https://github.com/tatsu-lab/alpaca_eval , 2023. 7\n[11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R \u00b4emi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Science , 378(6624):1092\u20131097, 2022. 1\n[12] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan.\nGuiding large language models via directional stimulus prompting. arXiv preprint\narXiv:2302.11520 , 2023. 3, 4\n[13] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\nFlorencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming\nBao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman,\nGreg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell,\n20", "start_char_idx": 2429, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48e08847-6812-4e4d-b4d5-5ab64f2522d2": {"__data__": {"id_": "48e08847-6812-4e4d-b4d5-5ab64f2522d2", "embedding": null, "metadata": {"page_label": "21", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "995c9372-b1db-4f00-abee-b7e6d05abeee", "node_type": "4", "metadata": {"page_label": "21", "file_name": "principled_instructions.pdf"}, "hash": "34aa697bf1e75e6864f0ac6f420577c0ee268903a02d891f56eb6735eb9804ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "467303d4-fb12-448e-b45c-90a201f0b31d", "node_type": "1", "metadata": {}, "hash": "5578c18308dd9dad82dba2bf71a75b055a48129ab3b942d2d06b94d6abc91c78", "class_name": "RelatedNodeInfo"}}, "text": "Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che\nChang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen,\nBen Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah\nCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville,\nArka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna\nEloundou, David Farhi, Liam Fedus, Niko Felix, Sim \u00b4on Posada Fishman, Juston Forte,\nIsabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni,\nGabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan\nGreene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga,\nShantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin,\nShino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar\nKanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\n\u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger,\nVishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy,\nChak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez,\nRyan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov,\nYaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta,\nJacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan\nMorikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M \u00b4ely, Ashvin\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail\nPavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly\nPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae,\nAditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted,\nHenri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry,\nHeather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie\nSimens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie\nStaudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,\nMadeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,\nNick Turley, Jerry Tworek,", "start_char_idx": 0, "end_char_idx": 3086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "467303d4-fb12-448e-b45c-90a201f0b31d": {"__data__": {"id_": "467303d4-fb12-448e-b45c-90a201f0b31d", "embedding": null, "metadata": {"page_label": "21", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "995c9372-b1db-4f00-abee-b7e6d05abeee", "node_type": "4", "metadata": {"page_label": "21", "file_name": "principled_instructions.pdf"}, "hash": "34aa697bf1e75e6864f0ac6f420577c0ee268903a02d891f56eb6735eb9804ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48e08847-6812-4e4d-b4d5-5ab64f2522d2", "node_type": "1", "metadata": {"page_label": "21", "file_name": "principled_instructions.pdf"}, "hash": "53fd43d44a4d3bbf5032a9180065dee34d296e21e8f3f9890ac1aefcc1cd55aa", "class_name": "RelatedNodeInfo"}}, "text": "Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted,\nHenri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry,\nHeather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie\nSimens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie\nStaudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,\nMadeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,\nNick Turley, Jerry Tworek, Juan Felipe Cer \u00b4on Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea V oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan\nWard, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,\nMatt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu,\nQiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia\nZhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical\nreport, 2023. 1, 3\n[14] Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong Zhang.\nPlum: Prompt learning using metaheuristic. arXiv preprint arXiv:2311.08364 , 2023. 3\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018. 3\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. 3\n21", "start_char_idx": 2461, "end_char_idx": 4167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2425038-ccdc-4205-accf-8d343e7a54e0": {"__data__": {"id_": "b2425038-ccdc-4205-accf-8d343e7a54e0", "embedding": null, "metadata": {"page_label": "22", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e6520bd-457e-4508-8d1f-afa42713b628", "node_type": "4", "metadata": {"page_label": "22", "file_name": "principled_instructions.pdf"}, "hash": "6979654e269a024e9729fffc72dba0840a045e4cca5cbfd8f74fe34cf59534b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd5a594c-1d3a-4798-8089-811601aeb6c0", "node_type": "1", "metadata": {}, "hash": "0c402db9681732f0a9530d4d17946d1dbf5ebe7f98383968e4fc63543d9f3847", "class_name": "RelatedNodeInfo"}}, "text": "[17] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Ruther-\nford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den\nDriessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes\nWelbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins,\nAntonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena\nBuchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Lau-\nrent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,\nMaria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer,\nOriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights\nfrom training gopher. CoRR , abs/2112.11446, 2021. 3\n[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. CoRR , abs/1910.10683, 2019. 3\n[19] Zhiqiang Shen, Sondos Mahmoud Bsharat, and Aidar Myrzakhan. Atlas: A llm inquiry\nprinciple benchmark. Preprint , 2024. 7\n[20] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh.\nAutoprompt: Eliciting knowledge from language models with automatically generated\nprompts, 2020.", "start_char_idx": 0, "end_char_idx": 1982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd5a594c-1d3a-4798-8089-811601aeb6c0": {"__data__": {"id_": "fd5a594c-1d3a-4798-8089-811601aeb6c0", "embedding": null, "metadata": {"page_label": "22", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e6520bd-457e-4508-8d1f-afa42713b628", "node_type": "4", "metadata": {"page_label": "22", "file_name": "principled_instructions.pdf"}, "hash": "6979654e269a024e9729fffc72dba0840a045e4cca5cbfd8f74fe34cf59534b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2425038-ccdc-4205-accf-8d343e7a54e0", "node_type": "1", "metadata": {"page_label": "22", "file_name": "principled_instructions.pdf"}, "hash": "c7164a24cbc11070506117efe3f5ad39bc8cebd3e3f244a20403f911db92de6f", "class_name": "RelatedNodeInfo"}}, "text": "CoRR , abs/2112.11446, 2021. 3\n[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. CoRR , abs/1910.10683, 2019. 3\n[19] Zhiqiang Shen, Sondos Mahmoud Bsharat, and Aidar Myrzakhan. Atlas: A llm inquiry\nprinciple benchmark. Preprint , 2024. 7\n[20] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh.\nAutoprompt: Eliciting knowledge from language models with automatically generated\nprompts, 2020. 3\n[21] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui\nYu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Sil-\nver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese,\nJilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy,\nMichael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm\nReynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford,\nErica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim\nKrikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana \u00a8\u0131s White, Anders\nAndreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha\nKhalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev,\nYi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Na-\ntalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard,\nAdam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan\nGarrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William\nWong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunx-\nuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad,\nAle Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter,\nEmanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin,\nDiego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri `a Puigdom `enech\nBadia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal,\nGabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming\nGu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El\nShafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie\nXiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy,\nSmit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting\n22", "start_char_idx": 1393, "end_char_idx": 4144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a41bbc87-6edf-4218-bf6e-9afa22d55526": {"__data__": {"id_": "a41bbc87-6edf-4218-bf6e-9afa22d55526", "embedding": null, "metadata": {"page_label": "23", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e8bad1c-437e-412d-a454-0705a14a0f9b", "node_type": "4", "metadata": {"page_label": "23", "file_name": "principled_instructions.pdf"}, "hash": "8e4b2d4478b9268a3b8f7a68a680bf8d345e4f3ba5ccf2689494650a0258caf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71b2a6ef-fe3a-4de0-8775-a5f560b07902", "node_type": "1", "metadata": {}, "hash": "2a2a617e3151a2b530988ffc371e52a48d6961965ef19af20ef26489990d3cf3", "class_name": "RelatedNodeInfo"}}, "text": "Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu,\nCindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys,\nThibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex\nKaskasoli, S \u00b4ebastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa,\nDmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson,\nPranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia\nLi, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth\nCole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi \u00b4nska, Vitaly Nikolaev, Pablo\nSprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro,\nGaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,\nRaoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha\nShrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu,\nDaniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova,\nShashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe\nZhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim \u00b4enez, Legg Yeung,\nHanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal,\nRachel Saputro, Kiran V odrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick\nFernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka\nAgrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin\nChang, Paul Komarek, Ross McIlroy, Mario Lu \u02c7ci\u00b4c, Guodong Zhang, Wael Farhan, Michael\nSharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao,\nSiamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani\nAgrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren\nMaggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Tay-\nlor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian\nRiedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil\nHoulsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa\nLee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay\nGhemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma,\nPolina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu,\nXiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha \u00a8el Lopez Kaufman, Simon\nTokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Sid-\ndhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko\nRoy, Zizhao Zhang, Ada Ma,", "start_char_idx": 0, "end_char_idx": 2806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71b2a6ef-fe3a-4de0-8775-a5f560b07902": {"__data__": {"id_": "71b2a6ef-fe3a-4de0-8775-a5f560b07902", "embedding": null, "metadata": {"page_label": "23", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e8bad1c-437e-412d-a454-0705a14a0f9b", "node_type": "4", "metadata": {"page_label": "23", "file_name": "principled_instructions.pdf"}, "hash": "8e4b2d4478b9268a3b8f7a68a680bf8d345e4f3ba5ccf2689494650a0258caf8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a41bbc87-6edf-4218-bf6e-9afa22d55526", "node_type": "1", "metadata": {"page_label": "23", "file_name": "principled_instructions.pdf"}, "hash": "aa1d536f43fd71a1876e2c47e2b6b94f4effa89b0cc02368f29c531909cae1e7", "class_name": "RelatedNodeInfo"}}, "text": "Mateo Wirth, Lisa\nLee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay\nGhemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma,\nPolina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu,\nXiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha \u00a8el Lopez Kaufman, Simon\nTokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Sid-\ndhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko\nRoy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko,\nChih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie\nMuir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen,\nSholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang,\nJustin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj \u00a8osund, S \u00b4ebastien Cevey, Zach Gleicher,\nThi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos\nAisopos, L \u00b4eonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri `a\nRecasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin\nFrye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan\nDyer, V \u00b4\u0131ctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil\nMustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross\nHemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe\nThacker, C \u00b8a\u02d8glar \u00a8Unl\u00a8u, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi,\nPiyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi,\nToby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan\nDafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie\nPellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi\n23", "start_char_idx": 2255, "end_char_idx": 4237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06b31233-f6e1-4ca4-8999-acbef7dd45f9": {"__data__": {"id_": "06b31233-f6e1-4ca4-8999-acbef7dd45f9", "embedding": null, "metadata": {"page_label": "24", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "52fcc4bb-6801-40e5-93ba-13e2696bf579", "node_type": "4", "metadata": {"page_label": "24", "file_name": "principled_instructions.pdf"}, "hash": "59b93c6f7b390705866be2bf80199dd13bee014debfd4cdd97ff096d16f0c673", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9542d8dd-52eb-455f-bf18-194ff9535f6c", "node_type": "1", "metadata": {}, "hash": "c38a1f08ec50cfe7bb6e20bc9ca19e6bcc7b6cc210b7d212867003996556552d", "class_name": "RelatedNodeInfo"}}, "text": "Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le\nHou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre\nMoufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane,\nPratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant\nMisra, Cheng Li, Nemanja Raki \u00b4cevi\u00b4c, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk\nOh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen,\nGamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko,\nPatrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam\nSadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan,\nEdouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan,\nPam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael\nAzzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz\nMohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane\nPark, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe\nDong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen,\nJohnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu\nMao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan,\nEvan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen,\nIzhak Shafran, Michael Fink, Alfonso Casta \u02dcno, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj\nRybi \u00b4nski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi\nGierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei,\nHarsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu,\nLu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu,\nRicardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der\nWeide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan\nYang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayaku-\nmar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng,\nKatherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz,\nSarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo,\nZiqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le,\nElena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Or-\ngad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins,\nRobert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R \u00b4emi\nLeblond, Vikas Yadav,", "start_char_idx": 0, "end_char_idx": 2838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9542d8dd-52eb-455f-bf18-194ff9535f6c": {"__data__": {"id_": "9542d8dd-52eb-455f-bf18-194ff9535f6c", "embedding": null, "metadata": {"page_label": "24", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "52fcc4bb-6801-40e5-93ba-13e2696bf579", "node_type": "4", "metadata": {"page_label": "24", "file_name": "principled_instructions.pdf"}, "hash": "59b93c6f7b390705866be2bf80199dd13bee014debfd4cdd97ff096d16f0c673", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06b31233-f6e1-4ca4-8999-acbef7dd45f9", "node_type": "1", "metadata": {"page_label": "24", "file_name": "principled_instructions.pdf"}, "hash": "6c0fab7412f409e12ca78a71b21be4cfe76ba8af9100036d1405437581b8a846", "class_name": "RelatedNodeInfo"}}, "text": "Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng,\nKatherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz,\nSarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo,\nZiqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le,\nElena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Or-\ngad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins,\nRobert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R \u00b4emi\nLeblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix\nFischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou,\nAlek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick,\nColton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra,\nJiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang,\nPraseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov,\nNate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan,\nVlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth\nMudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James\nLee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos,\nJohn Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze\nCheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily\nCaveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic,\nWeize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller,\nTian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel\nToyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John\nCarpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika\n24", "start_char_idx": 2253, "end_char_idx": 4269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64ab7323-71b1-4fe4-ac7f-9cc1ff00064b": {"__data__": {"id_": "64ab7323-71b1-4fe4-ac7f-9cc1ff00064b", "embedding": null, "metadata": {"page_label": "25", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60b32a50-fc63-4f10-8cb0-637b1a162155", "node_type": "4", "metadata": {"page_label": "25", "file_name": "principled_instructions.pdf"}, "hash": "101f04ae6e8773e6cdf83a1d27b36aa671e1707450a7aac0d5f49c9acae39487", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d6477e5-2b08-4ac1-8e25-97e433d55f23", "node_type": "1", "metadata": {}, "hash": "45f849e60d43e83a2474ba88b38880bce6a5ee534ba255ffb760dd52ba40bf17", "class_name": "RelatedNodeInfo"}}, "text": "Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du,\nChloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John\nWieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am \u00b4elie H \u00b4eliou,\nNing Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich,\nNorbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin\nStrudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo\nLee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan\nHoltmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka,\nQijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim P \u02dcoder, Steven Zheng, Francesco\nPongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti,\nRagha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi,\nJoe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie\nLi, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam,\nRob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero,\nBrona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik\nJacobsson, Alireza Ghaffarkhah, Morgane Rivi `ere, Alanna Walton, Cl \u00b4ement Crepy, Alicia\nParrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan,\nClaudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna\nKlimczak-Pluci \u00b4nska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolic-\nchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison\nReid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe,\nAnastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir\nGloberson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya\nBhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj\nKhare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei,\nRuoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak,\nDoug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma,\nTaylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika,\nKeyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David\nMadras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han\nZhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang\nXu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue,", "start_char_idx": 0, "end_char_idx": 2793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d6477e5-2b08-4ac1-8e25-97e433d55f23": {"__data__": {"id_": "3d6477e5-2b08-4ac1-8e25-97e433d55f23", "embedding": null, "metadata": {"page_label": "25", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60b32a50-fc63-4f10-8cb0-637b1a162155", "node_type": "4", "metadata": {"page_label": "25", "file_name": "principled_instructions.pdf"}, "hash": "101f04ae6e8773e6cdf83a1d27b36aa671e1707450a7aac0d5f49c9acae39487", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64ab7323-71b1-4fe4-ac7f-9cc1ff00064b", "node_type": "1", "metadata": {"page_label": "25", "file_name": "principled_instructions.pdf"}, "hash": "e8982cdda716ec9b67d246bc7d180a673a20c30918e5a538a21e989e39ba492a", "class_name": "RelatedNodeInfo"}}, "text": "Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak,\nDoug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma,\nTaylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika,\nKeyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David\nMadras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han\nZhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang\nXu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue,\nSherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger,\nEmmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Golden-\nson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki,\nChrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe\nRahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk\nLee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr,\nMia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green,\nDemis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A family\nof highly capable multimodal models, 2023. 3\n[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth \u00b4ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\nOpen and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 3\n[23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\n25", "start_char_idx": 2213, "end_char_idx": 4212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7ebabdc-3b3a-4f6d-8ecc-d3b25c35ee9c": {"__data__": {"id_": "c7ebabdc-3b3a-4f6d-8ecc-d3b25c35ee9c", "embedding": null, "metadata": {"page_label": "26", "file_name": "principled_instructions.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a0b8d18-6403-430b-9cfb-c745d54795dd", "node_type": "4", "metadata": {"page_label": "26", "file_name": "principled_instructions.pdf"}, "hash": "4d576d713791a82e3038e880d46014d9876599ea091eddcf3c5e5851cc61a90a", "class_name": "RelatedNodeInfo"}}, "text": "Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne\nLachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie\nKambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 3\n[24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels, 2023. 4\n[25] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf\nElnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. A prompt pattern catalog to\nenhance prompt engineering with chatgpt, 2023. 3, 4\n[26] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023. 7\n[27] Denny Zhou, Nathanael Sch \u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models, 2023. 3, 4\n26", "start_char_idx": 0, "end_char_idx": 1576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"147be114-6e64-4606-ab9c-727f38f96358": {"node_ids": ["47c4d19c-d72c-4d84-848c-e6c7726c4bbf"], "metadata": {"page_label": "1", "file_name": "principled_instructions.pdf"}}, "78e661ae-b3eb-45be-99cb-578ec0debb15": {"node_ids": ["0cd1a1b5-d0f8-45a3-9337-8a471845d961", "b0bf8545-494e-431c-ba5b-6483082b3164"], "metadata": {"page_label": "2", "file_name": "principled_instructions.pdf"}}, "a18b3dfb-2358-4d53-8858-dbafae882a2b": {"node_ids": ["1d7fd207-e8a5-47c8-b203-fbc116caba2e"], "metadata": {"page_label": "3", "file_name": "principled_instructions.pdf"}}, "dcf90697-212e-4a7e-9910-c2659c06c401": {"node_ids": ["ee7e21da-5822-4593-a744-1a90b5f86176"], "metadata": {"page_label": "4", "file_name": "principled_instructions.pdf"}}, "db1731c1-39e5-409a-a6a1-124d56f1c9d7": {"node_ids": ["833fdb77-3416-4035-9efa-2825805532e3"], "metadata": {"page_label": "5", "file_name": "principled_instructions.pdf"}}, "1ad4a6cb-a2da-4558-9d9d-83dbcb8e125e": {"node_ids": ["a8fa3638-3ab8-465f-b227-97cdc5edcccf"], "metadata": {"page_label": "6", "file_name": "principled_instructions.pdf"}}, "6a88dc97-8d0e-42f9-a2a4-222d373f7a56": {"node_ids": ["3bef6d8f-73da-4fb4-ae56-75272e7215d1"], "metadata": {"page_label": "7", "file_name": "principled_instructions.pdf"}}, "c2db5b72-6ae8-49c3-a32b-14bd4925be2e": {"node_ids": ["48d8a2d5-54e8-41b4-83f2-c40bf3d08d2b"], "metadata": {"page_label": "8", "file_name": "principled_instructions.pdf"}}, "a250b327-fb6f-4a03-8ef3-0e1cbfbf2c4f": {"node_ids": ["c492d0c3-4461-4d67-8eb7-1b68d5532e94"], "metadata": {"page_label": "9", "file_name": "principled_instructions.pdf"}}, "95c59b4e-7eb7-4391-a5d8-911e5874e19b": {"node_ids": ["59d67118-5878-4283-a2ae-ad62b1fa24a8"], "metadata": {"page_label": "10", "file_name": "principled_instructions.pdf"}}, "a9deb209-06ed-4fda-8e46-a5d43ce9b2a1": {"node_ids": ["2c70108e-192b-4e44-8a03-1d42bea658cf"], "metadata": {"page_label": "11", "file_name": "principled_instructions.pdf"}}, "7421f293-a6fc-453d-84a5-0d219894eeda": {"node_ids": ["6ba7d6df-b14c-490a-a084-d01253ffdfc2"], "metadata": {"page_label": "12", "file_name": "principled_instructions.pdf"}}, "ed7c1c6b-10fd-4cc9-ba23-4df720836825": {"node_ids": ["f8c4bee8-cba8-495c-88d2-3341191704f4"], "metadata": {"page_label": "13", "file_name": "principled_instructions.pdf"}}, "effda0de-c169-4ba6-a1e8-db3793f30041": {"node_ids": ["0b569a7e-88ab-450d-9450-aa3c47b07e0d", "5a0cfe07-b635-4fed-a364-57ed20036d10"], "metadata": {"page_label": "14", "file_name": "principled_instructions.pdf"}}, "7c7eddad-b587-4efa-86a2-43ae5b3bfaf5": {"node_ids": ["ff2ff41a-dfe0-4d23-98cf-ebc90da29ecd"], "metadata": {"page_label": "15", "file_name": "principled_instructions.pdf"}}, "89715caf-6735-4607-b3a2-a854ecc8c9d7": {"node_ids": ["ee3710e9-3993-4b3c-ae47-2a2b43b82ca4"], "metadata": {"page_label": "16", "file_name": "principled_instructions.pdf"}}, "a329cf5c-f7b9-4958-be14-a2eb7aff7221": {"node_ids": ["42757665-8d6e-4cc8-9504-028b5cf45b34"], "metadata": {"page_label": "17", "file_name": "principled_instructions.pdf"}}, "bd17e473-6af2-411d-a0e3-0811abfc616c": {"node_ids": ["b25fb3fe-a3cd-48f0-b32c-2da21c10ea08"], "metadata": {"page_label": "18", "file_name": "principled_instructions.pdf"}}, "c6ec5370-d419-4bf8-8238-1399f040f019": {"node_ids": ["4aefdb6b-4714-4b2c-8de0-27a50dfeea63"], "metadata": {"page_label": "19", "file_name": "principled_instructions.pdf"}}, "b695dc0e-24a0-4e0e-b7a5-73308ddf08e0": {"node_ids": ["17230e4b-5db2-4f94-92b5-16ed5609718b", "6030409e-ef31-46b2-9dd0-1d9e958f5a83"], "metadata": {"page_label": "20", "file_name": "principled_instructions.pdf"}}, "995c9372-b1db-4f00-abee-b7e6d05abeee": {"node_ids": ["48e08847-6812-4e4d-b4d5-5ab64f2522d2", "467303d4-fb12-448e-b45c-90a201f0b31d"], "metadata": {"page_label": "21", "file_name": "principled_instructions.pdf"}}, "3e6520bd-457e-4508-8d1f-afa42713b628": {"node_ids": ["b2425038-ccdc-4205-accf-8d343e7a54e0", "fd5a594c-1d3a-4798-8089-811601aeb6c0"], "metadata": {"page_label": "22", "file_name": "principled_instructions.pdf"}}, "2e8bad1c-437e-412d-a454-0705a14a0f9b": {"node_ids": ["a41bbc87-6edf-4218-bf6e-9afa22d55526", "71b2a6ef-fe3a-4de0-8775-a5f560b07902"], "metadata": {"page_label": "23", "file_name": "principled_instructions.pdf"}}, "52fcc4bb-6801-40e5-93ba-13e2696bf579": {"node_ids": ["06b31233-f6e1-4ca4-8999-acbef7dd45f9", "9542d8dd-52eb-455f-bf18-194ff9535f6c"], "metadata": {"page_label": "24", "file_name": "principled_instructions.pdf"}}, "60b32a50-fc63-4f10-8cb0-637b1a162155": {"node_ids": ["64ab7323-71b1-4fe4-ac7f-9cc1ff00064b", "3d6477e5-2b08-4ac1-8e25-97e433d55f23"], "metadata": {"page_label": "25", "file_name": "principled_instructions.pdf"}}, "0a0b8d18-6403-430b-9cfb-c745d54795dd": {"node_ids": ["c7ebabdc-3b3a-4f6d-8ecc-d3b25c35ee9c"], "metadata": {"page_label": "26", "file_name": "principled_instructions.pdf"}}}}